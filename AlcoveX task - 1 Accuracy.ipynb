{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-sse2ZPNN-N",
        "outputId": "58ebbf19-9198-410e-a814-039982f79bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "356/356 - 11s - loss: 6.4953 - accuracy: 0.0498 - val_loss: 7.0908 - val_accuracy: 0.0460 - 11s/epoch - 32ms/step\n",
            "Epoch 2/30\n",
            "356/356 - 8s - loss: 5.8463 - accuracy: 0.0556 - val_loss: 7.2369 - val_accuracy: 0.0629 - 8s/epoch - 23ms/step\n",
            "Epoch 3/30\n",
            "356/356 - 8s - loss: 5.6950 - accuracy: 0.0702 - val_loss: 7.3534 - val_accuracy: 0.0680 - 8s/epoch - 22ms/step\n",
            "Epoch 4/30\n",
            "356/356 - 8s - loss: 5.4961 - accuracy: 0.0865 - val_loss: 7.3753 - val_accuracy: 0.0759 - 8s/epoch - 22ms/step\n",
            "Epoch 5/30\n",
            "356/356 - 8s - loss: 5.2380 - accuracy: 0.1163 - val_loss: 7.3500 - val_accuracy: 0.0738 - 8s/epoch - 23ms/step\n",
            "Epoch 6/30\n",
            "356/356 - 7s - loss: 4.9823 - accuracy: 0.1347 - val_loss: 7.4133 - val_accuracy: 0.0852 - 7s/epoch - 20ms/step\n",
            "Epoch 7/30\n",
            "356/356 - 8s - loss: 4.7823 - accuracy: 0.1518 - val_loss: 7.5136 - val_accuracy: 0.0861 - 8s/epoch - 23ms/step\n",
            "Epoch 8/30\n",
            "356/356 - 8s - loss: 4.6163 - accuracy: 0.1640 - val_loss: 7.6310 - val_accuracy: 0.0889 - 8s/epoch - 22ms/step\n",
            "Epoch 9/30\n",
            "356/356 - 7s - loss: 4.4704 - accuracy: 0.1735 - val_loss: 7.6945 - val_accuracy: 0.0877 - 7s/epoch - 19ms/step\n",
            "Epoch 10/30\n",
            "356/356 - 8s - loss: 4.3419 - accuracy: 0.1832 - val_loss: 7.8402 - val_accuracy: 0.0963 - 8s/epoch - 23ms/step\n",
            "Epoch 11/30\n",
            "356/356 - 8s - loss: 4.2187 - accuracy: 0.1939 - val_loss: 7.9523 - val_accuracy: 0.0891 - 8s/epoch - 22ms/step\n",
            "Epoch 12/30\n",
            "356/356 - 8s - loss: 4.1042 - accuracy: 0.2034 - val_loss: 8.0240 - val_accuracy: 0.0931 - 8s/epoch - 22ms/step\n",
            "Epoch 13/30\n",
            "356/356 - 8s - loss: 3.9936 - accuracy: 0.2152 - val_loss: 8.1428 - val_accuracy: 0.0963 - 8s/epoch - 23ms/step\n",
            "Epoch 14/30\n",
            "356/356 - 7s - loss: 3.8864 - accuracy: 0.2255 - val_loss: 8.2279 - val_accuracy: 0.0938 - 7s/epoch - 20ms/step\n",
            "Epoch 15/30\n",
            "356/356 - 8s - loss: 3.7822 - accuracy: 0.2358 - val_loss: 8.3076 - val_accuracy: 0.0921 - 8s/epoch - 23ms/step\n",
            "Epoch 16/30\n",
            "356/356 - 8s - loss: 3.6786 - accuracy: 0.2489 - val_loss: 8.3618 - val_accuracy: 0.0926 - 8s/epoch - 23ms/step\n",
            "Epoch 17/30\n",
            "356/356 - 7s - loss: 3.5806 - accuracy: 0.2605 - val_loss: 8.4621 - val_accuracy: 0.0886 - 7s/epoch - 21ms/step\n",
            "Epoch 18/30\n",
            "356/356 - 8s - loss: 3.4841 - accuracy: 0.2745 - val_loss: 8.5656 - val_accuracy: 0.0889 - 8s/epoch - 23ms/step\n",
            "Epoch 19/30\n",
            "356/356 - 8s - loss: 3.3897 - accuracy: 0.2846 - val_loss: 8.6674 - val_accuracy: 0.0907 - 8s/epoch - 23ms/step\n",
            "Epoch 20/30\n",
            "356/356 - 8s - loss: 3.2974 - accuracy: 0.2984 - val_loss: 8.6925 - val_accuracy: 0.0875 - 8s/epoch - 22ms/step\n",
            "Epoch 21/30\n",
            "356/356 - 8s - loss: 3.2080 - accuracy: 0.3142 - val_loss: 8.7953 - val_accuracy: 0.0908 - 8s/epoch - 23ms/step\n",
            "Epoch 22/30\n",
            "356/356 - 7s - loss: 3.1223 - accuracy: 0.3232 - val_loss: 8.8523 - val_accuracy: 0.0903 - 7s/epoch - 21ms/step\n",
            "Epoch 23/30\n",
            "356/356 - 8s - loss: 3.0380 - accuracy: 0.3371 - val_loss: 8.9842 - val_accuracy: 0.0910 - 8s/epoch - 23ms/step\n",
            "Epoch 24/30\n",
            "356/356 - 8s - loss: 2.9571 - accuracy: 0.3488 - val_loss: 9.0262 - val_accuracy: 0.0919 - 8s/epoch - 23ms/step\n",
            "Epoch 25/30\n",
            "356/356 - 7s - loss: 2.8811 - accuracy: 0.3617 - val_loss: 9.0989 - val_accuracy: 0.0919 - 7s/epoch - 20ms/step\n",
            "Epoch 26/30\n",
            "356/356 - 8s - loss: 2.8076 - accuracy: 0.3731 - val_loss: 9.1804 - val_accuracy: 0.0917 - 8s/epoch - 23ms/step\n",
            "Epoch 27/30\n",
            "356/356 - 9s - loss: 2.7366 - accuracy: 0.3850 - val_loss: 9.2478 - val_accuracy: 0.0884 - 9s/epoch - 25ms/step\n",
            "Epoch 28/30\n",
            "356/356 - 7s - loss: 2.6697 - accuracy: 0.3969 - val_loss: 9.3394 - val_accuracy: 0.0886 - 7s/epoch - 21ms/step\n",
            "Epoch 29/30\n",
            "356/356 - 8s - loss: 2.6047 - accuracy: 0.4065 - val_loss: 9.4095 - val_accuracy: 0.0886 - 8s/epoch - 23ms/step\n",
            "Epoch 30/30\n",
            "356/356 - 8s - loss: 2.5421 - accuracy: 0.4173 - val_loss: 9.5071 - val_accuracy: 0.0865 - 8s/epoch - 23ms/step\n",
            "Accuracy: 0.864523\n",
            "1/1 [==============================] - 0s 444ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Upload the dataset\n",
        "with open('/content/pg5200.txt', 'r') as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "# Tokenize the data into sequences of words\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "seq_length = 3\n",
        "sequences = []\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    seq = tokens[i-seq_length:i]\n",
        "    sequences.append(seq)\n",
        "\n",
        "# Create the input and output data\n",
        "X = []\n",
        "y = []\n",
        "for seq in sequences:\n",
        "    X.append(seq[:-1])\n",
        "    y.append(seq[-1])\n",
        "\n",
        "# Create the vocabulary\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "for i, word in enumerate(set(tokens)):\n",
        "    word_to_index[word] = i\n",
        "    index_to_word[i] = word\n",
        "vocab_size = len(word_to_index)\n",
        "\n",
        "# Convert the input and output data to numerical values\n",
        "X_num = np.zeros((len(X), seq_length-1))\n",
        "y_num = np.zeros((len(y), 1))\n",
        "for i, (seq, label) in enumerate(zip(X, y)):\n",
        "    X_num[i, :] = [word_to_index[word] for word in seq]\n",
        "    y_num[i, :] = word_to_index[label]\n",
        "\n",
        "# Embed the input data\n",
        "embedding_size = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=seq_length-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Convert the output to one-hot encoding\n",
        "y_one_hot = to_categorical(y_num, num_classes=vocab_size)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(X_num))\n",
        "X_train, X_val = X_num[:train_size], X_num[train_size:]\n",
        "y_train, y_val = y_one_hot[:train_size], y_one_hot[train_size:]\n",
        "\n",
        "# Train the LSTM model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=2, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the LSTM model\n",
        "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*10))\n",
        "\n",
        "# Use the LSTM model to generate text\n",
        "seed_text = \"As nobody could\"\n",
        "for i in range(10):\n",
        "    # Create the input sequence\n",
        "    token_list = nltk.word_tokenize(seed_text)\n",
        "    token_list = token_list[-seq_length+1:]\n",
        "    input_seq = np.array([word_to_index[word] for word in token_list])[np.newaxis, :]\n",
        "\n",
        "    # Predict the next word\n",
        "    predicted_probs = model.predict(input_seq)[0]\n"
      ]
    }
  ]
}